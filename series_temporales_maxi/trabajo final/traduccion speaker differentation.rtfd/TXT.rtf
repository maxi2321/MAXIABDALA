{\rtf1\ansi\ansicpg1252\cocoartf2639
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red251\green2\blue7;\red0\green0\blue255;\red0\green0\blue0;
\red33\green255\blue6;\red251\green2\blue255;}
{\*\expandedcolortbl;;\cssrgb\c100000\c14913\c0;\cssrgb\c1680\c19835\c100000;\cssrgb\c0\c0\c0;
\cssrgb\c0\c97680\c0;\cssrgb\c100000\c25279\c100000;}
\paperw11900\paperh16840\margl1440\margr1440\vieww16520\viewh15160\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx4730\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs96 \cf2 Diferenciaci\'f3n De Oradores Mediante Un Autocodificador Convolucional\
\

\fs48 \cf3 Resumen:\cf4  en este trabajo, una soluci\'f3n de aprendizaje profundo para diferenciar las voces de los altavoces en el audio dadas las dos fuentes de micr\'f3fono se presenta como un paso hacia la soluci\'f3n del problema de la fiesta de c\'f3ctel.\cb5  Se entren\'f3 a un autocodificador convolucional utilizando un peque\'f1o tama\'f1o de muestra de datos para asociar fragmentos de audio con etiquetas categ\'f3ricas\cb1 . Los fragmentos de audio recopilados como parte de este trabajo se utilizaron para entrenar y evaluar el modelo. \cb5 El audio se convirti\'f3 en representaci\'f3n de cepstrum de frecuencia mel antes de la clasificaci\'f3n\cb1 . Los datos procesados colectivamente se etiquetaron de acuerdo con la persona o la colecci\'f3n de personas que hablaban. El modelo se entren\'f3 y evalu\'f3 utilizando datos de dos, tres, cuatro, cinco y seis categor\'edas. El resultado fue un modelo que reconoce cuando diferentes personas est\'e1n hablando en una conversaci\'f3n de 2 personas, 3 personas, 4 personas, 5 personas y 6 personas con una precisi\'f3n del 99,29 %, 97,62 %, 96,43 %, 93,43 % y 88,1 %, respectivamente. Se presentan comparaciones experimentales entre las cinco versiones del modelo.\
\
\cf3 T\'e9rminos de \'edndice:\cf4  aprendizaje profundo, red neuronal convolucional, separaci\'f3n de voz, procesamiento de se\'f1ales\
\
\cf3 I. Introducci\'f3n\cf4 \
\
Este documento describe la investigaci\'f3n, el dise\'f1o y la implementaci\'f3n de una soluci\'f3n de aprendizaje profundo \cb5 para diferenciar las voces de los altavoces en el audio procedente de dos micr\'f3fonos simult\'e1neamente\cb1 . El arte anterior, como [1], se centr\'f3 en el uso de una soluci\'f3n de aprendizaje profundo para realizar la separaci\'f3n de altavoces dada una se\'f1al de audio mixta. Este trabajo se centra en el uso de una soluci\'f3n de aprendizaje profundo para realizar la diferenciaci\'f3n o clasificaci\'f3n de los altavoces dadas las se\'f1ales de audio en las que un solo orador est\'e1 hablando a la vez. El modelo desarrollado en este trabajo se presenta como un primer paso para resolver el problema del c\'f3ctel.\
\
El objetivo de este trabajo era evaluar c\'f3mo funcionar\'eda un modelo de aprendizaje autom\'e1tico para resolver la diferencia de altavoces en audio cuando el n\'famero de oradores y el tama\'f1o del conjunto de datos era mayor o menor. \cb5 Bucketization\cb1 , que se conoce com\'fanmente como binning multivariado o  An\'e1lisis de datos multivariados, es el proceso de extraer solo caracter\'edsticas relevantes de los datos dados para clasificarlos [2]. En este trabajo, se extrajeron caracter\'edsticas relevantes de los fragmentos de audio para clasificar los fragmentos para que pertenecieran a un orador espec\'edfico (altavoz). En los datos multidimensionales, no todas las caracter\'edsticas son relevantes a la hora de clasificar las muestras de datos [2]. Como tal, es necesario un enfoque en el que solo queden caracter\'edsticas interesantes [2]. Se han utilizado m\'e9todos tradicionales como el an\'e1lisis de componentes principales (PCA) y la escala multidimensional (MDS) para la extracci\'f3n de caracter\'edsticas relevantes dados datos multidimensionales [2]. Este trabajo describe el uso de una soluci\'f3n de aprendizaje autom\'e1tico para realizar la extracci\'f3n de caracter\'edsticas de per-form, as\'ed como la bucketizaci\'f3n autom\'e1ticamente. Se han implementado muchos tipos diferentes de modelos de aprendizaje autom\'e1tico para resolver varios problemas y desaf\'edos relacionados con las se\'f1ales de audio. \cb5 El modelo de aprendizaje autom\'e1tico utilizado en este trabajo fue un \cb6 autocodificador convolucional (CAE).\cb1 \
\
\cf3 II. Fondo\cf4 \
\
\cb5 Los autocodificadores (AE) se componen de dos partes, un codificador y un decodificador\cb1 . El codificador reducir\'e1 las dimensiones de la entrada a una representaci\'f3n de espacio latente, mientras que el decodificador intentar\'e1 reconstruir los datos comprimidos [3]. El muestreo descendente se utiliza durante la etapa de codificador y el muestreo descendente durante la etapa de decodificador para comprimir y reconstruir los datos de entrada, respectivamente [4]. Los AE, espec\'edficamente los CAE, se han utilizado en muchas aplicaciones que implican el uso de datos de imagen. Los CAE han sido efectivos en la extracci\'f3n de caracter\'edsticas visuales de las im\'e1genes de entrada y, como resultado, han permitido la generaci\'f3n de representaciones de espacio latente mucho m\'e1s precisas en comparaci\'f3n con los AE tradicionales [4]. La figura 1 proporciona una representaci\'f3n visual de la estructura de un AE.\
\
Los CAE se basan en AE est\'e1ndar donde se utilizan capas convolucionales para la codificaci\'f3n y/o decodificaci\'f3n [4]. CAEs preservan
\fs24 \cf0 {{\NeXTGraphic Captura de Pantalla 2024-05-24 a la(s) 12.47.53.png \width7560 \height5420 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\

\fs48 Localidad espacial, ya que los pesos se comparten entre todas las ubicaciones de entrada [4]. \cb5 Durante el entrenamiento, se utiliza la propagaci\'f3n posterior para calcular el gradiente de la funci\'f3n de error. Esto permite actualizar los pesos de la red utilizando un algoritmo de descenso de gradiente como Adadelta [4].\
\cb1 \
III. ARTE ANTERIOR\
\
\cb5 Las representaciones de cepstrum de frecuencia Mel (MFCC) han dominado el campo del reconocimiento de voz en t\'e9rminos de representaci\'f3n de caracter\'edsticas, ya que proporcionan una forma compacta de la representaci\'f3n del espectro de amplitud del audio [5]. Los MFCC proporcionan una representaci\'f3n espectral a corto plazo de las caracter\'edsticas de audio [5]. El trabajo en [5] encontr\'f3 que los MFCC no ten\'edan un efecto negativo en los algoritmos de discriminaci\'f3n del habla y la m\'fasica cuando se usaban como paso de preprocesamiento para la extracci\'f3n de caracter\'edsticas.\
\cb1 \
Los CAE se han utilizado como una soluci\'f3n a muchos problemas diferentes que van desde la clasificaci\'f3n hasta el desnido. Un ejemplo de aplicaci\'f3n de CAE en datos de desnudo, es en el campo de la desnudo de im\'e1genes m\'e9dicas [6]. El trabajo en [6] describe c\'f3mo el uso de datos de tama\'f1o de muestra peque\'f1o junto con un CAE puede desa\'f1ar de manera eficiente las im\'e1genes m\'e9dicas.\
\
Los CAE tambi\'e9n se han aplicado a los problemas basados en la clasificaci\'f3n como se describe en [7]. En [7], se utiliz\'f3 un CAE para clasificar im\'e1genes de radar de apertura sint\'e9tica (SAR) de alta resoluci\'f3n que est\'e1n contaminadas por el ruido de moteado [7]. El trabajo en [7] describe c\'f3mo los CAE son capaces de extraer caracter\'edsticas, as\'ed como de clasificar las im\'e1genes SAR autom\'e1ticamente, eliminando la necesidad de una extracci\'f3n de caracter\'edsticas manual y que consume mucho tiempo.\
\
El campo de la separaci\'f3n y clasificaci\'f3n de oradores tambi\'e9n ha hecho uso de algoritmos de aprendizaje autom\'e1tico [8]. En [8], el objetivo era resolver el problema del c\'f3ctel con t\'e9cnicas y algoritmos modernos de aprendizaje autom\'e1tico. \cb5 Se utiliz\'f3 un clasificador de red neuronal de regresi\'f3n general como soluci\'f3n para el problema del c\'f3ctel [8]. En [8] se utiliz\'f3 una red neuronal artificial multicapa para clasificar el enfoque de atenci\'f3n de un oyente en un entorno de varios altavoces, dadas las caracter\'edsticas extra\'eddas de los datos del electroencefalograma. El modelo se evalu\'f3 en funci\'f3n de la precisi\'f3n de la clasificaci\'f3n, la sensibilidad, la especificidad y el tiempo de c\'e1lculo. Los resultados fueron de alrededor del 99 % de precisi\'f3n de clasificaci\'f3n,\
\cb1 \
98,9 % de sensibilidad, 99,1 % de especificidad y alrededor de 8 segundos de tiempo de computaci\'f3n [8].\
\
Otro ejemplo de aplicaci\'f3n de una soluci\'f3n de aprendizaje autom\'e1tico en la separaci\'f3n de altavoces se describe en [9]. Al igual que [8], en [9] se propuso una soluci\'f3n moderna de aprendizaje autom\'e1tico para resolver el problema del c\'f3ctel. En [9], \cb5 se propuso una soluci\'f3n en la que se utiliz\'f3 la invarianza y el entrenamiento de la permutaci\'f3n a nivel de expresi\'f3n para entrenar una red neuronal recurrente de memoria bidireccional a largo plazo [9]. El modelo resultante fue capaz de mejorar la relaci\'f3n se\'f1al-distorsi\'f3n, as\'ed como la inteligibilidad objetiva extendida a corto plazo para las desafiantes relaciones se\'f1al-ruido en el audio de varios altavoces [9].\cb1 \
\
\cf3 IV. SISTEMA DESARROLLADO\
\cf0 \
En el estado de la t\'e9cnica (por ejemplo, [8], [9]), se implementaron soluciones de aprendizaje autom\'e1tico no CAE para resolver la separaci\'f3n de altavoces en el audio de varios altavoces. Sin embargo, los sistemas desarrollados requer\'edan un preprocesamiento de audio pesado, grandes conjuntos de datos y demostraron ser computacionalmente intensivos. Este trabajo describe el uso de una soluci\'f3n moderna de aprendizaje autom\'e1tico para resolver la diferenciaci\'f3n de altavoces en el audio de varios altavoces. Se dise\'f1\'f3 y desarroll\'f3 una soluci\'f3n que requiere un preprocesamiento de audio m\'ednimo, conjuntos de datos de tama\'f1o de muestra peque\'f1o y que era computacionalmente eficiente. Se entren\'f3 y evalu\'f3 un modelo CAE en el origen de audio a partir de dos micr\'f3fonos simult\'e1neamente. El objetivo era evaluar la precisi\'f3n del CAE a medida que aumenta el n\'famero de cubos en la salida de la red. La expectativa era que una combinaci\'f3n espec\'edfica de calidad del conjunto de datos, tama\'f1o del conjunto de datos y tama\'f1o del cubo proporcionar\'eda la mejor precisi\'f3n, recuperaci\'f3n y precisi\'f3n por parte del CAE para diferenciar los altavoces en audio. A medida que aumente el tama\'f1o del cubo en la salida del CAE, es decir, aumentar el n\'famero de neuronas de salida, la matriz de confusi\'f3n resultante ser\'e1 m\'e1s grande. Con N cubos, se construir\'eda una matriz N x N. La expectativa era que, sin importar el tama\'f1o y la calidad del conjunto de datos, a medida que aumentara el n\'famero de cubos (es decir, N se hace m\'e1s grande), la precisi\'f3n del modelo eventualmente disminuir\'eda.\
\
En el estado de la t\'e9cnica (por ejemplo, [6], [7]), la parte del decodificador del CAE se utiliz\'f3 para reconstruir los datos reducidos a la representaci\'f3n del espacio latente. Sin embargo, en este trabajo, el decodificador del CAE utiliz\'f3 las caracter\'edsticas relevantes en los datos codificados, los datos que se proporcionaron como entrada al decodificador, con el fin de diferenciar los altavoces en el audio original coloc\'e1ndolos dentro de cubos. Como tal, el decodificador en este trabajo era simplemente una red neuronal densa (DNN) en lugar de una convolucional en la que se proporcionaban los datos codificados aplanados como entrada.\
\
\cb5 La estructura del CAE implementada en este trabajo es bastante sencilla. El codificador estaba compuesto por una capa de entrada seguida de tres capas convolucionales, una capa de agrupaci\'f3n y una capa de ca\'edda. El m\'e9todo de agrupaci\'f3n m\'e1s utilizado en las soluciones modernas de aprendizaje autom\'e1tico, Max-Pooling, se utiliz\'f3 para realizar la reducci\'f3n de la dimensionalidad [10]. La reducci\'f3n de la dimensi\'f3n se realiza com\'fanmente en la etapa de preprocesamiento de las aplicaciones de agrupaci\'f3n y clasificaci\'f3n [11]. En este trabajo, la reducci\'f3n de la dimensionalidad fue parte de la etapa de an\'e1lisis de datos en lugar de la etapa de preprocesamiento. La salida reducida del decodificador se aplan\'f3 con una capa aplanada. El la capa de aplanamiento permite aplanar los mapas de caracter\'edsticas de la salida del decodificador combin\'e1ndolos [12]. Los datos aplanados se proporcionaron entonces como entrada al decodificador, que estaba compuesto por capas densas con capas de ca\'edda en el medio. Se utilizaron capas de ca\'edda, ya que ayudan a evitar que el modelo se sobreajuste [13]. La estructura del CAE implementado en este trabajo se describe con m\'e1s detalle en la Tabla I y la Figura 2.\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\fs24 \cf0 \cb1 {{\NeXTGraphic Captura de Pantalla 2024-05-24 a la(s) 12.49.02.png \width7020 \height11300 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\fs48 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx4730\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \cb5 La funci\'f3n de activaci\'f3n utilizada para todas las capas, aparte de la capa de salida, fue la unidad lineal rectificada (ReLU), ya que es eficiente desde el punto de vista computacional [14]. La funci\'f3n de activaci\'f3n utilizada para la capa de salida fue softmax, ya que el decodificador se utiliz\'f3 para la clasificaci\'f3n de varias clases. La funci\'f3n de p\'e9rdida fue una entrop\'eda cruzada categ\'f3rica con Adadelta como optimizador. Se eligieron la activaci\'f3n de Softmax y la funci\'f3n de penalizaci\'f3n de entrop\'eda cruzada, ya que hay un emparejamiento natural entre ellos [15]. Adadelta se utiliz\'f3 como algoritmo de optimizaci\'f3n, ya que especificar una tasa de aprendizaje es innecesario, ya que se ha eliminado de la regla de actualizaci\'f3n del algoritmo [16]. Como tal, el n\'famero de hiperpar\'e1metros se redujo, ya que el modelo requer\'eda menos ajuste manual de la tasa de aprendizaje.\cb1 \
\
Antes de entrenar y probar el modelo, los conjuntos de datos de audio se procesaron previamente de la siguiente manera. El trabajo en [5], demostrado\
Que los MFCC pueden proporcionar una representaci\'f3n espectral a corto plazo de las caracter\'edsticas de audio sin afectar negativamente a los algoritmos de discriminaci\'f3n del habla y la m\'fasica. Como tal, con el fin de mejorar la eficiencia computacional del sistema desarrollado en este trabajo sin una reducci\'f3n en la precisi\'f3n, los datos de entrada \cb5 se procesaron previamente transformando cada fragmento de audio en una representaci\'f3n MFCC con una longitud de ventana de 2048 y una longitud de salto de 512. Para calcular los MFCC de los fragmentos de audio utilizados en este trabajo, se utiliz\'f3 el paquete python Librosa. Librosa es un paquete de Python que se usa com\'fanmente en el campo de la m\'fasica para recuperar informaci\'f3n de los datos de audio [17]. La conversi\'f3n de los datos de audio en representaci\'f3n MFCC dio como resultado datos 2D similares a la imagen que eran similares a los datos de [6] [7]. Para cada archivo de audio, el preprocesamiento dio como resultado un tensor de 20 x 11. En este caso, la funci\'f3n MFCC de Librosa calcul\'f3 20 MFCC en 11 fotogramas. Cada par de representaciones tensoras MFCC de fragmentos de audio id\'e9nticos, una de cada micr\'f3fono, se concatenaron para servir como entrada al CAE desarrollado en este trabajo. Como tal, la forma de los datos de entrada para el modelo fue (40, 11).\cb1 \
\
\cf3 V. PREPARACI\'d3N DE DATOS\cf0 \
\
Los datos de entrenamiento y prueba se recopilaron utilizando dos microtel\'e9fonos simult\'e1neamente. Se recogieron fragmentos de audio de un total de 6 oradres menores de 30 a\'f1os, 3 de los cuales eran hombres y 3 de los cuales eran mujeres. Los hablantes tambi\'e9n eran una mezcla de hablantes nativos y no nativos de ingl\'e9s. Durante la grabaci\'f3n, los altavoces se colocaron a una distancia de 2 m de ambos micr\'f3fonos, con los dos micr\'f3fonos a una distancia de 1 m de distancia. Cada micr\'f3fono estaba conectado a una Raspberry Pi 3 Modelo V1.2 que se ejecutaba en Rasbian OS, donde se iniciaban, se deten\'edan y guardaban las grabaciones. Esto permiti\'f3 fragmentos de audio separados id\'e9nticos de cada micr\'f3fono.\
\
A los oradores se les dieron las mismas transcripciones del conjunto de datos Common Voice de Mozilla. Cada orador lee la misma lista de oraciones, donde cada frase fue grabada por cada micr\'f3fono y guardada en un archivo WAV. Cada par de archivos WAV generados se consider\'f3 como una sola muestra en el conjunto de datos utilizado en este trabajo. Cada archivo WAV ten\'eda una frecuencia de muestreo de 44100 Hz y una duraci\'f3n de 10 segundos con el nivel de ruido en la habitaci\'f3n a un promedio de 47dB durante las grabaciones. Se recogieron un total de 82 grabaciones para cada altavoz de cada micr\'f3fono para el conjunto de datos de entrenamiento, y se recogieron un total de 12 grabaciones para cada altavoz de cada micr\'f3fono para el conjunto de datos de prueba. Ambos conjuntos de datos se organizaron y etiquetaron de acuerdo con el altavoz presente en el fragmento de audio y el micr\'f3fono de origen utilizado para grabar ese fragmento. Cada altavoz representar\'eda un cubo en la salida del modelo.\
\
\cf3 VI. RESULTADOS\
\cf0 \
\cb5 El modelo se entren\'f3 con un tama\'f1o de lote de 64 sobre 80 \'e9pocas, es decir, 80 pases completos del conjunto de datos de entrenamiento a trav\'e9s del CAE, utilizando un conjunto de datos de 82 fragmentos de audio por altavoz. El conjunto de datos de entrenamiento se dividi\'f3 entre un subconjunto de entrenamiento de 70 fragmentos de audio por altavoz y un subconjunto de 10 fragmentos de audio por altavoz para evaluar el modelo. La precisi\'f3n del modelo se evalu\'f3 mediante la construcci\'f3n de matrices de confusi\'f3n donde se calcularon los valores de verdaderos positivos (TP), verdaderos negativos (TN), falsos positivos (FP) y falsos negativos (FN). La precisi\'f3n y el recuerdo se calcularon utilizando TP, FN y FP. La Tabla II proporciona el recuerdo, la precisi\'f3n y la precisi\'f3n de cada matriz de confusi\'f3n.\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\fs24 \cf0 \cb1 {{\NeXTGraphic Captura de Pantalla 2024-05-24 a la(s) 12.50.18.png \width7500 \height3480 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\fs48 \cf0 Table II demuestra claramente que a medida que aumenta el n\'famero de d\'f3lares, la precisi\'f3n del modelo disminuye. El siguiente diagrama proporciona una visualizaci\'f3n de la disminuci\'f3n de la precisi\'f3n del modelo a medida que aumenta el n\'famero de cubos.\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\fs24 \cf0 {{\NeXTGraphic Captura de Pantalla 2024-05-24 a la(s) 12.50.48.png \width7300 \height4880 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\fs48 \cf0 \cb5 Los resultados extra\'eddos de las matrices de confusi\'f3n generadas indican que un CAE puede diferenciar los altavoces con audio procedente de dos micr\'f3fonos simult\'e1neamente.\cb1  Los resultados tambi\'e9n indican que cuanto mayor sea el n\'famero de cubos en los que se entren\'f3 el modelo, menos preciso era el modelo. Como tal, se ha confirmado la hip\'f3tesis de que a medida que aumenta el n\'famero de cubos, la precisi\'f3n del modelo eventualmente disminuir\'eda. Sin en modo, se debe tener en cuenta el n\'famero limitado de muestras de prueba. La calidad y la cantidad de los datos tienen un impacto directo en el rendimiento de los algoritmos de aprendizaje autom\'e1tico [18]. \cb5 Como tal, con el fin de verificar los resultados extra\'eddos de las matrices de confusi\'f3n, tambi\'e9n se utiliz\'f3 la validaci\'f3n cruzada de k-fold para evaluar el modelo. En este trabajo, el modelo se evalu\'f3 utilizando 10 pliegues, donde el rendimiento del modelo se evalu\'f3 en el pliegue de validaci\'f3n sostenido [19]. La tabla III y la figura 4 proporcionan los resultados de validaci\'f3n cruzada de k veces.\
\cb1 \
La Tabla III y la Figura 4 demuestran adem\'e1s que un CAE puede diferenciar los altavoces en el audio. Sin embargo, como se se\'f1al\'f3 anteriormente en base a los resultados de las matrices de confusi\'f3n, la Tabla III y la Figura 4 tambi\'e9n demuestran que como el n\'famero de 
\fs24 {{\NeXTGraphic Captura de Pantalla 2024-05-24 a la(s) 12.51.35.png \width7560 \height8360 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\

\fs48 oradores aumenta, la precisi\'f3n del modelo disminuye. Los resultados obtenidos mediante la validaci\'f3n cruzada de k-fold tambi\'e9n parecen ser ligeramente mejores que los obtenidos con matrices de confusi\'f3n. En la validaci\'f3n cruzada de k veces, el conjunto de datos de entrenamiento se divide al azar en k subconjuntos, o pliegues, donde el modelo se entrena en cada subconjunto excepto en uno. El rendimiento del modelo se eval\'faa en el subconjunto de validaci\'f3n sostenido [19]. En este trabajo, el modelo se prob\'f3 en 10 pliegues a partir de un conjunto de datos de 82 muestras. El modelo se entren\'f3 10 veces y se evalu\'f3 cada vez en el subconjunto o pliegue aleatorio. A continuaci\'f3n, se evalu\'f3 el rendimiento del modelo tomando el promedio de los resultados de precisi\'f3n de cada pliegue de evaluaci\'f3n. \cb5 Como tal, dado el conjunto de datos limitado utilizado en este trabajo, la validaci\'f3n cruzada de k-fold proporciona una menor varianza y reduce el sesgo en el subconjunto de prueba en comparaci\'f3n con un solo conjunto de retenci\'f3n al evaluar el modelo.\
\cb1 \
En el preprocesamiento de datos, los tensores resultantes de cada mi-crophone se concatenaron y se proporcionaron como entrada al modelo. \cb5 Cambiar la forma de entrada del modelo de (40, 11) a (20,11) y proporcionar el tensor resultante de un solo micr\'f3fono, permiti\'f3 evaluar el rendimiento del modelo cuando se utiliza un solo micr\'f3fono. La tabla IV y la figura 5 proporcionan los resultados de validaci\'f3n cruzada de k veces cuando se utiliza un solo micr\'f3fono y demuestran claramente que el modelo funcion\'f3 mejor cuando se le da audio de dos micr\'f3fonos.\cb1 \
\
\cf3 VII. DISCUSI\'d3N Y CONCLUSI\'d3N\
\cf0 \
\cb5 Los hallazgos de este trabajo demuestran claramente que un CAE se puede utilizar con precisi\'f3n para diferenciar los altavoces dado un peque\'f1o tama\'f1o de muestra de audio recogido de dos micr\'f3fonos simult\'e1neamente. Con la validaci\'f3n cruzada, el modelo fue capaz de diferenciar entre dos, tres, cuatro, cinco y seis altavoces
\fs24 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \cb1 {{\NeXTGraphic Captura de Pantalla 2024-05-24 a la(s) 12.52.08.png \width7680 \height8420 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\fs48 \cf0 \cb5 Clases con una precisi\'f3n de alrededor del 99 %, 97 %, 96 %, 93 % y 88 %, respectivamente. Los resultados tambi\'e9n indican que a medida que aumenta el n\'famero de clases de oradores, la precisi\'f3n del modelo disminuye. Esto confirma la hip\'f3tesis esbozada al comienzo de este trabajo de que no importa el tama\'f1o y la calidad del conjunto de datos, a medida que aumenta el n\'famero de cubos, la precisi\'f3n del modelo eventualmente disminuir\'eda. Sin embargo, dados los resultados de la precisi\'f3n, cuando se les dieron hasta 5 altavoces diferentes, el modelo todav\'eda fue capaz de diferenciar entre los altavoces en el audio recogido de dos micr\'f3fonos simult\'e1neamente con una precisi\'f3n de m\'e1s del 90 %. Los resultados tambi\'e9n demuestran que el modelo funcion\'f3 mejor cuando se le dio audio de dos micr\'f3fonos simult\'e1neamente en comparaci\'f3n con un solo micr\'f3fono. Utilizando la validaci\'f3n cruzada y con datos de audio preprocesados recopilados de un solo micr\'f3fono, el modelo fue capaz de diferenciar entre dos, tres, cuatro, cinco y seis clases de altavoces con una precisi\'f3n de aproximadamente el 94 %, el 95 %, el 93 %, el 85 % y el 82 %, respectivamente. Como tal, con el uso de dos micr\'f3fonos simult\'e1neamente para recopilar el audio del altavoz, los resultados demuestran que el uso de un CAE para resolver la clasificaci\'f3n de varios altavoces en el audio es tanto una soluci\'f3n eficiente como una soluci\'f3n de alto rendimiento.\
\cb1 \
Una investigaci\'f3n adicional sobre la ca\'edda en la precisi\'f3n del modelo a medida que aumenta el n\'famero de cubos, podr\'eda incluir el uso de conjuntos de datos de tama\'f1o de muestra m\'e1s grande a medida que aumenta el n\'famero de cubos. Es posible que un aumento en el n\'famero de muestras de entrenamiento cuando se le da un mayor n\'famero de cubos pueda eliminar la ca\'edda en la precisi\'f3n del modelo. Otros intentos de mejorar el rendimiento del modelo pueden incluir el aumento de la calidad del conjunto de datos de entrenamiento o el uso de otras t\'e9cnicas de preprocesamiento de sonido que no sean MFCC. MFCC elimina una gran cantidad de informaci\'f3n de la onda de audio original, utilizando otras t\'e9cnicas que conServir m\'e1s informaci\'f3n, como la transformaci\'f3n de Fourier a corto plazo (STFT, por sus siglas en ingl\'e9s) que puede permitir que el modelo diferencie con mayor precisi\'f3n entre los altavoces en el audio.\
\
VIII. AGRADECIMIENTOS\
\
Reconocemos el apoyo del Consejo de Investigaci\'f3n de Ciencias Naturales e Ingenier\'eda de Canad\'e1 (NSERC) y de la Corporaci\'f3n Unificada de Inteligencia Inform\'e1tica de Canad\'e1.\
\
REFERENCIAS\
\
[1] Y. Isik, J. L. Roux, Z. Chen, S. Watanabe y J. R. Hershey, "Separaci\'f3n de m\'faltiples altavoces de un solo canal mediante agrupaci\'f3n profunda", arXiv preprint arXiv:1607.02173, 2016.\
\
[2] K. Lu, H.-W. Shen et al., "An\'e1lisis y visualizaci\'f3n de datos volum\'e9tricos multivariantes a trav\'e9s de la exploraci\'f3n del subespacio de abajo hacia arriba", en el Simposio de Visualizaci\'f3n del Pac\'edfico IEEE 2017 (PacificVis). IEEE, 2017, pp. 141-150.\
\
[3] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, y P.-A. Manzagol, "Autocodificadores de desnuido apilados: Aprendizaje de representaciones \'fatiles en una red profunda con un criterio de desnuido local", Journal of machine learning research, vol. 11, no. Dic, pp. 3371-3408, 2010.\
\
[4] J. Masci, U. Meier, D. Cires\uc0\u32 \u807 an, y J. Schmidhuber, "Codificadores autom\'e1ticos apilados para la extracci\'f3n de caracter\'edsticas jer\'e1rquicas", en la Conferencia Internacional sobre Redes Neuronales Artificiales. Springer, 2011, pp. 52-59.\
\
[5] B. Logan et al., "Mel frequency cepstral coefficients for music model- ing." en ISMIR, vol. 270, 2000, pp. 1-11.\
\
[6] L. Gondara, "Denoido de imagen m\'e9dica utilizando autocodificadores de denoido convolucional", en Talleres de Miner\'eda de Datos (ICDMW), 16a Conferencia Internacional IEEE 2016 sobre. IEEE, 2016, pp. 241-246.\
\
[7] J. Geng, J. Fan, H. Wang, X. Ma, B. Li, y F. Chen, "Clasificaci\'f3n de im\'e1genes sar de alta resoluci\'f3n a trav\'e9s de autocodificadores convolucionales profundos", IEEE Geoscience and Remote Sensing Letters, vol. 12, no. 11, pp. 2351-2355, 2015.\
\
[8] P. Shree, P. Swami, V. Suresh y T. K. Gandhi, "Una nueva t\'e9cnica para identificar la selecci\'f3n de atenci\'f3n en un entorno dic\'f3tico", en la Conferencia de la India (INDICON), IEEE Annual 2016. IEEE, 2016, pp. 1-5.\
\
[9] M. Kolb\'e6k, D. Yu, Z.-H. Tan y J. Jensen, "Separaci\'f3n conjunta y desnivel de ruido del discurso ruidoso de m\'faltiples habladores utilizando redes neuronales recurrentes y entrenamiento de invariantes de permutaci\'f3n", en Machine Learning for Signal Processing (MLSP), 2017 IEEE 27th International Workshop on. IEEE, 2017, pp. 1-6.\
\
[10] S. Albawi, T. A. Mohammed y S. Al-Zawi, "Comprensi\'f3n de una red neuronal convolucional", en Ingenier\'eda y Tecnolog\'eda (ICET), Conferencia Internacional de 2017 sobre. IEEE, 2017, pp. 1-6.\
\
[11] B. Yang, X. Fu y N. D. Sidiropoulos, "Aprendizaje de rasgos ocultos: an\'e1lisis factorial conjunto y agrupaci\'f3n latente", IEEE Transactions on Signal Processing, vol. 65, no. 1, pp. 256-269, 2017.\
\
[12] Y. Zheng, Q. Liu, E. Chen, Y. Ge, y J. L. Zhao, "Clasificaci\'f3n de series temporales utilizando redes neuronales convolucionales profundas multicanal", en la Conferencia Internacional sobre Gesti\'f3n de la Informaci\'f3n de la Era Web. Springer, 2014, pp. 298-310.\
\
[13] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever y R. Salakhut- dinov, "Dropout: una forma sencilla de evitar que las redes neuronales se ajusten en exceso", The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929-1958, 2014.\
\
[14] D.-A. Clevert, T. Unterthiner y S. Hochreiter, "Aprendizaje profundo en red r\'e1pido y preciso por unidades lineales exponenciales (elus)", arXiv preprint arXiv:1511.07289, 2015.\
\
[15] R. A. Dunne y N. A. Campbell, "Sobre el emparejamiento de las funciones de activaci\'f3n softmax y penalizaci\'f3n de entrop\'eda cruzada y la derivaci\'f3n de la funci\'f3n de activaci\'f3n softmax", en Proc. 8th Aust. Conf. sobre las redes neuronales, Melbourne, vol. 181. Citeseer, 1997, p. 185.\
\
[16] S. Ruder, "Una visi\'f3n general de los algoritmos de optimizaci\'f3n de descenso de gradiente, 2016", arXiv preprint arXiv:1609.04747, 2016.\
\
[17] B. McFee, C. Raffel, D. Liang, D. P. Ellis, M. McVicar, E. Battenberg, y O. Nieto, "librosa: An\'e1lisis de se\'f1ales de audio y m\'fasica en python", en Proceedings of the 14th python in science conference, 2015, pp. 18-25.\
\
[18] V. Sesiones y M. Valtorta, "Los efectos de la calidad de los datos en los algoritmos de aprendizaje autom\'e1tico". ICIQ, vol. 6, pp. 485-498, 2006.\
\
[19] T. Wong y N. Yang, "An\'e1lisis de dependencia de las estimaciones de precisi\'f3n en la validaci\'f3n cruzada k-fold", IEEE Transactions on Knowledge and Data Engineering, vol. 29, no. 11, pp. 2417-2427, noviembre de 2017.}